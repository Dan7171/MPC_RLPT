# 1/6/24

1. Important Scripts (some of them with our chages):
    
    1.1 storm/examples/franka_reacher.py - this is the original script. Made by storm team.
    1.2 storm/examples/franka_reacher_for_rl.py - the main script "we'll use" (elias said this). Made by Elias.
    1.3 storm/examples/franka_reacher_for_rl_comparison.py - a debugging script (by elias).Made by Elias. Its goal is to see the difference in arm motion under different parameter setting BUT in the same world. 
    1.4 storm/content/configs/mpc/franka_reacher.yml (elias changed it, tbd: explain its goal)
    1.5 storm/storm_kit/gym/core.py (elias changed it, tbd: explain its goal)
    1.6 storm/storm_kit/mpc/* (elias changed files in every subdirectory)
    1.7 storm.storm_kit/mpc/cost/* (Made by storm team, maybe elias made some changes.)- A very important part for parameter tuning. Contains the cost funtions related to the hyper parameters we wish to tune.

1.1 storm/examples/franka_reacher.py

    1. This script is a Python program that uses NVIDIA's Isaac Gym library to simulate and control a robotic arm. 
    2.The main function of the script, mpc_robot_interactive, sets up a simulated environment and allows interactive control of a robot using Model Predictive Control (MPC). 
    3.mpc_robot_interactive():
        Steps:

        Load robot, task, and world configurations:
        Load YAML parameters for robot, task, and world.

        Initialize simulation and device:
        Setup simulation parameters and determine CUDA or CPU usage.

        Create robot simulation object:
        Instantiate RobotSim with given parameters and device.

        Spawn robot in environment:
        Add robot to the simulation environment.

        Spawn camera in environment:
        Add a camera for visualization in the environment.

        Initialize robot pose and transformation:
        Set initial pose and transformation for the robot.

        Create world instance:
        Instantiate World object with simulation parameters.

        Spawn interactive objects:
        Add objects like target and mug to the simulation.

        Initialize MPC controller:
        Setup MPC controller with task, robot, and world files.

        Get initial robot state:
        Retrieve the initial state of the robot.

        Update MPC goal parameters:
        Set the desired goal state for the MPC controller.

        Main simulation loop:
        Continuously update simulation and control robot.

        Step simulation:
        Advance the simulation by one time step.

        Update goal if necessary:
        Adjust the goal position/orientation if it changes.

        Get control command from MPC:
        Compute control commands using the MPC controller.

        Send commands to robot:
        Apply computed position commands to the robot.

        Visualize and log results:
        Draw trajectories and print error information.

        Handle KeyboardInterrupt:
        Gracefully exit the loop on interrupt signal.

        Close MPC controller:
        Clean up and close the MPC controller.



1.3 storm/examples/franka_reacher_for_rl_comparison.py

    1. written by elias
    2. The code idea was taking mpc_robot_interactive() from franka_reacher.py and adjust it to our needs by making a class out of it MpcRobotInteractive.
    3.class MpcRobotInteractive
    core concepts:
        1. docstring:            
            """
            This class is for controlling the arm base and simulator.
            It contains the functions for RL learning.
            """
        2. In __init__() it does allt he initiatiation work that was in mpc_robot_interactive() + Adding next variables:
            # RL variables
            self.end_effector_pos = None
            self.end_effector_quat = None
            self.goal_pose = [0,0,0,0,0,0,1]
            self.arm_configuration = None
            self.objects_configuration = None

            # File variables
            self.vis_ee_target = True
            self.robot_file = self.args.robot + '.yml'
            self.task_file = self.args.robot + '_reacher.yml'
            self.world_file = 'collision_primitives_3d_origina2.yml'

            # Simulator variables
            self.pose = None # Goal pose in simulation
        3. __main__
            - Executing episodes using ***reseting of the environment*** unlike original franka_reacher.
            - Using step() and reset() functions. 
            - In this part we can also tune our parameters.
            - Every step() call makes a step of the robot int he real world! (every episode is composed from a straight of "mpc steps").
            - Every reset() is resetting the environment at the end of the steps of a (real) episode (switching to a new environment probably)
            - reset(self):
                """
                Change location of objects in environment and target goal
                Input
                    - objects: dict {object_type: [pos, dimension]}
                    - goal_pos: numpy array [7], (x,y,z, quaternion) of target
                Output
                    - observation: 2 numpy arrays [object dimensions and positions], [q_pos, ee_pos, ee_quat]
                """
            - def step(self, cost_params, mpc_params, i):
                """
                Update arm parameters. cost_params are the parameters for the mpc cost function. mpc_params are the horizon and number of particles of the mpc.
                Input
                    - cost_params: dict {cost_name: weight}
                    - mpc_params: dict {horizon: num, num_particles: num}
                Output
                    - observation: 2 numpy arrays [object dimensions and positions], [q_pos, ee_pos, ee_quat, g_pos, g_quat]
                    - reward: float reward function for RL
                    - done: bool True, False. True if the arm reached te goal or if it is in an invalid configuration
                """
                # See important part elias added: self.mpc_control.update_costs() which updates the params!

            
            - This is how they put it all together (based on the same while loop at franka_reacher.py/mpc_robot_interactive() but this time with the explained above modifications elias did)


                ################
                __main__  start
                ################
                Mpc = MpcRobotInteractive(args, gym_instance)
                i = 0
                end_flag = True
                steps_in_one_epispde = 10 # dan
                while(i > -100):
                    # >>> Dan 
                    # Dan - to play with the non 0 cost params and see the affect.
                    # <<<
                    cost_params = {
                        "manipulability": 30, 
                        "stop_cost": 50, 
                        "stop_cost_acc": 0.0, 
                        "smooth": 0.0, 
                        "state_bound": 1000.0, 
                        "ee_vel": 0.0, 
                        "robot_self_collision" : 5000, 
                        "primitive_collision" : 5000, 
                        "voxel_collision" : 0
                        }
                    mpc_params = {
                        "horizon" : 90 , 
                        "particles" : 500 
                        } #dan
                    arm_configuration, goal_pose, end_effector_pos, end_effector_quat, done = Mpc.step(cost_params, mpc_params, i)
                    if end_flag:
                        start_time = time.time()
                        end_flag = False
                    end_time = 0
                    if i%steps_in_one_epispde == 0: 
                        end_time = time.time()
                        end_flag = True
                        elapsed_time = end_time - start_time
                        print(f"Execution time: {elapsed_time:.6f} seconds!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                        Mpc.reset()
                    if done:
                        break
                    i += 1
                
                ################
                __main__ over
                ################
                


