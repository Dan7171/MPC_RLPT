agent:
  model: # The Neural network 
    load_checkpoint: false
    checkpoint_path: 'BGU/Rlpt/trained_models/2025:01:06(Mon)22:47:44/model.pth' # None
    dst_dir: 'BGU/Rlpt/trained_models'
    include_etl: true
    state_representation: # input layer 
      robot_dofs_positions: true
      robot_dofs_velocities: true
      goal_pose: false 
      coll_objs: false
      robot_base_pos: false
      prev_action_idx: false
      pi_mppi_means: true
      pi_mppi_covs: false # currently disabled since it doesent change
    
  A: 
    cost_fn_space:
      goal_pose: [[1.0, 300.0],[300.0, 1.0]] # [[15,100]] # [[1.0, 300.0], [300.0, 1.0]] # weight of penalty for end-effector orientation/position error (0: orientation, 1: position)
      zero_vel: [0.0]
      zero_acc: [0.0]
      joint_l2: [0.0]
      robot_self_collision: [100, 1000] # [100, 1000] # weight of penalty for reaching too close to own parts (penalty weight)
      primitive_collision:  [100, 1000] # [100, 1000] # weight of penalty for reaching too close to obstacles in env (penalty weight)
      voxel_collision: [0.0]
      null_space: [1.0]
      manipulability: [30]
      ee_vel: [0.0]
      stop_cost: [[120.0, 1.5], [20.0, 4]] # [[100.0, 1.5]] # [[120.0, 1.5], [20.0, 4]] # weight of penalty for crossing max vel. (0: cost term weight, 1: acceleration limit)
      stop_cost_acc: [[0.0, 0.1]]
      smooth: [1.0]
      state_bound: [1000.0]

    mppi_space:
      horizon: [30, 120] # [30] # horizon length. warning: linearly increasing runtime.
      particles: [500,1000] # [500] # num of mpc samples (each at length horizon). warning: linearly increasing runtime 
      n_iters: [1, 2] # num of repeats on mpc sampling and policy updating (optimiziation). warning: linearly increasing runtime
  
  action_space:
    use_original_storm_params: false
    tuning_enabled: false # normal mode is true, false is for debug


  train_suit:
    gamma: 0.999
    batch_size: 128
    eps_decay: true
    default_eps: 0.01
    grad_clipping: false
    N: 100000 # replay buffer size 

  reward:
    goal_pos_thresh_dist: 0.01 # activation thresold of position part in goal state (distance in meters).  TODO: when working with poser reward, I think it'd be better to make it larger. 
    goal_rot_thresh_dist: 0.01 # activation threshold of orientation part in goal state.  TODO: when working with poser reward, I think it'd be better to make it larger. 
    col_thresh_dist: 0.05 # activation threshold of collision detection, This distance found as a sweet spot for collision detection. It Should bot be changed unless having a good reason.
    time_reward: linear # fixed # ("linear" (w * -time passed) or "fixed" (-1 on every time step))
    pose_reward: true # false means no direct reward on goal state, learning just from time to goal and collisions
    safety_w: 1000
    pose_w: 1000
    step_dur_w: 1
    terminal_collision: true
    terminal_goal: true

  training:
    run: true
    save_checkpoints: true
    n_episodes: 128
    max_ts: 1000
    sample_objs_every_episode: false
    sample_obj_locs_every_episode: false
    sample_goal_every_episode: false
    reset_to_initial_state_every_episode: true
    default_goal_pose: [-0.27, 0.3, 0.3, 0, 0.4, 0, 0.2] # in storm coordinates.  other options: [-0.37, -0.37, 0.3, 0, 2.5, 0, 1], #  behind robot: reachible from start pose at large H (i succeeded with 320), [-0.27, 0.3, 0.3, 0, 0.4, 0, 0.2], # right to robot: rechible from start pose [0.3, -0.47, 0.31, 0, 0, 0, 1], # left to robot: rotated upside down - reachible from start pose with no self/premitive collisions. Failing for no reason due to too high self collision weight (but no real self collision). 
    etl_file_name: 'training_etl.csv'

  testing:
    run: false
    n_episodes: 10
    max_ts: 1000
    sample_objs_every_episode: false
    sample_obj_locs_every_episode: false
    sample_goal_every_episode: false
    reset_to_initial_state_every_episode: true
    default_goal_pose: [-0.27, 0.3, 0.3, 0, 0.4, 0, 0.2] # in storm coordinates.  other options: [-0.37, -0.37, 0.3, 0, 2.5, 0, 1], #  behind robot: reachible from start pose at large H (i succeeded with 320), [-0.27, 0.3, 0.3, 0, 0.4, 0, 0.2], # right to robot: rechible from start pose [0.3, -0.47, 0.31, 0, 0, 0, 1], # left to robot: rotated upside down - reachible from start pose with no self/premitive collisions. Failing for no reason due to too high self collision weight (but no real self collision).
    etl_file_name: 'test_etl.csv'




cost_sniffer:
  gui: false
  save_costs: false

gui:
  render_ee_icons: true
  render_trajectory_lines: true

episode_plots:
  show: true 

profile_memory:
  include: false # true means use this tool to profile https://pytorch.org/blog/understanding-gpu-memory-1/ 
